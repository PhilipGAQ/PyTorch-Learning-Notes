# 梯度
`.backward()` 是 PyTorch 中的一个方法，用于计算张量的梯度。它通常与自动微分（Autograd）一起使用，在神经网络的反向传播过程中计算梯度。

在PyTorch中，自动微分机制能够根据计算图自动计算张量的梯度。计算图是由张量和操作组成的有向无环图，它记录了张量之间的计算关系。通过调用 `.backward()` 方法，可以自动计算图中每个叶子节点（即具有 `requires_grad=True`）的梯度。

以下是一个示例，展示了如何使用 `.backward()` 方法计算梯度：

```python
import torch

# 创建一个张量
x = torch.tensor([2.0, 3.0], requires_grad=True)

# 执行一些计算操作
y = x.sum()  # 对张量求和
z = y ** 2   # 对结果进行平方

# 计算梯度
z.backward()

# 获取梯度
gradient = x.grad

print(gradient)  # 输出: tensor([8., 8.])
```

在上述示例中，我们创建了一个张量 `x`，并设置 `requires_grad=True` 以启用梯度追踪。然后，我们对张量执行一些计算操作，包括对张量求和和平方。接下来，我们调用 `z.backward()` 来计算 `z` 对于 `x` 的梯度。最后，我们通过访问 `x.grad` 属性获取张量 `x` 的梯度值。

调用 `.backward()` 方法会根据计算图执行自动微分，并将梯度累积到叶子节点的 `.grad` 属性中。在上述示例中，`gradient` 变量将包含张量 `x` 的梯度值。

`.backward()` 方法在训练神经网络和使用梯度下降等优化算法时非常有用。它使得我们能够方便地计算张量的梯度，并利用这些梯度来更新模型的参数。